# CI. LLMの基礎

## 概要

Large Language Models（LLM）は、Transformerアーキテクチャを基盤とした深層学習モデルです。トークン（単語の最小単位）を入力として、次の単語を確率的に予測することで、自然言語理解と生成を実現します。AI駆動開発では、Transformer・トークン・温度・コンテキスト長・ファインチューニングの仕組みを理解することで、より効果的なLLM活用が可能になります。

---

## 1. Transformerアーキテクチャ

Transformerは、Self-Attention（自己注意メカニズム）により、入力文の各単語が他の単語とどう関連するかを学習します。従来のRNNと異なり、すべての単語を並列処理でき、長距離の依存関係を効率的に学習できます。位置エンコーディングにより、「The dog chased the cat」と「The cat chased the dog」の違いも正確に識別します。

**Transformerの処理フロー**
```
テキスト → [トークン化] → [埋め込み] → [Self-Attention] → [フィードフォーワード] → 出力
```

---

## 2. トークンとコスト

トークンはLLMの最小処理単位で、通常は単語より小さいサブワード単位で分割されます。APIコストはトークン数に応じて課金されるため、理解が重要です。

**トークン数の目安**
- 英語1単語 ≈ 1.3トークン
- 日本語1文字 ≈ 1トークン
- A4用紙1ページ ≈ 300-400トークン

各LLMのコンテキストウィンドウ（同時処理可能なトークン上限）：Claude 3.5 Sonnet/Haiku は200,000トークン（約500-600ページ）、GPT-4は128,000トークン、Gemini Ultraは1,000,000トークン。

---

## 3. 温度（Temperature）パラメータ

温度は出力のランダムさを制御します。温度0.0は決定論的（毎回同じ答え、正確な情報向け）、温度0.5はバランス型、温度1.0は標準、温度2.0以上はより創造的です。

**使い分けの例**
```javascript
temperature: 0.0   // 計算結果・コード → 確定的な答えが必要
temperature: 0.7   // 説明文・会話 → 多様性を持たせたい
temperature: 1.2   // 創作・ブレインストーミング → 創造性重視
```

---

## 4. コンテキスト長の活用

コンテキスト長が長いほど、過去の会話や多くの参考資料をLLMに与えることができます。効率的に活用するには、システムプロンプト（ペルソナ・ルール）を最初に提供し、その後に複数の質問を続けます。

---

## 5. ファインチューニング

ファインチューニングは、事前学習済みモデルを特定ドメインに適応させるプロセスです。企業独自の用語・形式・規制が重要な場合に有効ですが、訓練データが数千例以上必要です。一般的なタスクや訓練データが少ない場合はプロンプトエンジニアリングやRAGで対応します。

**手法の比較**
```
ファインチューニング → コスト高、適応能力高、高速
RAG              → コスト低、適応能力中、普通速度
プロンプト設計   → コスト最低、適応能力低、高速
```

---

## 6. トークン予測のメカニズム

LLMは各ステップで「これまでのトークン列から、次のトークンの確率分布を計算し、温度に応じて単語を選択」します。これを繰り返す自己回帰生成により、1トークンずつ出力を生成していきます。

---

## まとめ

- **Transformer**：Self-Attentionで文脈を正確に把握
- **トークン**：処理単位であり、コスト計算の基準
- **温度**：タスクに応じて0.0～1.2を調整
- **コンテキスト長**：効率的に複数情報を同時処理
- **ファインチューニング**：ドメイン特化が必須な場合の選択肢
- **予測メカニズム**：確率的な生成により柔軟な出力を実現

これらを理解することで、プロンプトエンジニアリングやRAGといった応用技術をより効果的に活用できます。
