# CL. RAG（検索拡張生成）

## 概要

RAG（Retrieval-Augmented Generation、検索拡張生成）は、LLMに「外部の知識ベースから関連情報を検索・取得してから回答を生成させる」技術です。最新情報や組織固有の資料を活用し、より正確で信頼性の高い回答を実現します。埋め込み（Embedding）・ベクトル検索・チャンク分割・リランキングが組み合わさり機能します。

---

## 1. RAGの基本構造

RAGは2段階プロセスです。従来のLLMは学習済み知識だけで回答（高速だが古い情報・幻覚のリスク）するのに対し、RAGは外部知識ベースを検索してから回答生成（複雑だが信頼性高い）します。

**RAGパイプライン**
```
準備フェーズ：文書登録 → チャンク分割 → 埋め込み → ベクトルDB保存
実行フェーズ：質問 → 埋め込み → ベクトル検索 → リランキング → LLM生成 → 回答
```

---

## 2. 埋め込み（Embedding）

埋め込みは、テキストを固定長の数値ベクトルに変換するプロセスです。意味的に似たテキストは、ベクトル空間で近い位置に配置されます。

**主要モデル**
```
text-embedding-3-small  → 1536次元、OpenAI、高速、汎用
text-embedding-3-large  → 3072次元、OpenAI、高精度
Multilingual-E5         → 1024次元、多言語対応
JINA-embedding          → 2048次元、テキスト長に強い
```

**実装の要点**

```python
from openai import OpenAI
import numpy as np

def embed_text(text):
    """テキストをベクトルに変換"""
    response = OpenAI().embeddings.create(
        input=text,
        model="text-embedding-3-small"
    )
    return np.array(response.data[0].embedding)

def cosine_similarity(vec1, vec2):
    """2つのベクトルの類似度を計算"""
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
```

---

## 3. ベクトル検索

ベクトル検索（semantic search）は、クエリをベクトルに変換し、ベクトルDBから最も近いテキストを取得します。従来のキーワード検索と異なり、意味的な関連性に基づいて検索できます。

**キーワード検索との違い**
```
キーワード検索    → 「プログラミング」+「言語」を含む文書 → ノイズあり
ベクトル検索      → 意味的に最も近い文書 → 精度高い
```

---

## 4. チャンク分割（Chunking）

長いドキュメントをそのまま埋め込むと、細かい情報が失われます。チャンク分割により、ドキュメントを適切なサイズに分割し、検索精度を向上させます。

**チャンクサイズの目安**
```
50～100単語      → 文や句の抽出
200～500単語     → Q&A、短い記事
500～1000単語    → ブログ、ドキュメント
1000+単語        → 技術文書、論文
```

**分割戦略**
- 固定サイズ分割：定数サイズで分割（オーバーラップで境界情報を保持）
- セマンティック分割：段落・見出しなど意味的な境界を尊重（より自然）

---

## 5. リランキング（Reranking）

ベクトル検索で取得した候補を、より精度の高い判定で再度ランク付けします。初期検索の近似的な類似度判定より、正確な関連性を算出できます。

**効果**
```
ベクトル検索スコア     → [0.88, 0.85, 0.82, 0.79, 0.76]
リランキング適用後     → [0.92, 0.78, 0.80, 0.75, 0.70]（順位変動）
→ より正確な関連情報をLLMに提供
```

**実装例**

```python
from sentence_transformers import CrossEncoder

reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

def rerank_results(query, documents, top_k=3):
    """ベクトル検索結果をリランキング"""
    pairs = [[query, doc] for doc in documents]
    scores = reranker.predict(pairs)
    ranked = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)
    return [{"text": doc, "score": score} for doc, score in ranked[:top_k]]
```

---

## 6. RAGパイプライン実装

RAGシステムは、ドキュメント登録フェーズと質問応答フェーズから構成されます。登録時にチャンク・埋め込み・保存、質問時に検索・リランキング・生成を行います。

**完全なフロー**
```
登録：文書 → チャンク分割 → 各チャンクを埋め込み → ベクトルDB保存
応答：質問 → 埋め込み → ベクトル検索(top_k=10) → リランキング(top_k=3) → LLMに渡す
```

---

## 7. 幻覚（Hallucination）対策

RAGでは、プロンプトに「提供資料のみを参考に、資料にない情報は追加しない」と明記することが重要です。信頼度スコア（リランキングスコアの平均）を推定し、低い場合は「情報が見つかりません」と返すことも有効です。

---

## 8. RAG導入の判断基準

**RAGが必要な場合**
- ドメイン固有の最新情報が必要
- 社内マニュアル・ドキュメント参照が必要
- 出典を明示する必要がある
- 信頼性が重要（医療、法律など）

**RAGが不要な場合**
- 一般的な知識質問のみ
- リアルタイム性が不要
- システム複雑性を避けたい
- コスト最小化が優先

**実装オプション**
```
基本RAG             → コスト低、精度中、複雑度低、MVP向け
RAG+リランキング    → コスト中、精度高、複雑度中、本番向け
複数検索+融合       → コスト高、精度高、複雑度高、超高精度要求時
```

---

## まとめ

**重点項目**
- **埋め込み**：テキストを意味的ベクトルに変換、類似度計算の基盤
- **ベクトル検索**：意味的関連性に基づく効率的な検索
- **チャンク分割**：検索精度向上のための文書分割戦略
- **リランキング**：検索結果の精度向上
- **幻覚対策**：外部資料厳密参照、虚偽情報削減
- **パイプライン統合**：各要素の協調運用

これらを適切に組み合わせることで、正確性と信頼性に優れたAIアシスタントが実現できます。
